{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_model",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qahi1Tu7lJ7",
        "colab_type": "code",
        "outputId": "1fb1ca11-bafe-4e70-d1a3-b1f853791b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "#!pip install tf-nightly-gpu-2.0-preview\n",
        "!pip3 install soundfile\n",
        "!pip3 install python_speech_features\n",
        "\n",
        "\n",
        "import urllib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# download data: \n",
        "!cp -r \"/content/drive/My Drive/WordList/mfcc_final\" .\n",
        "\n",
        "!cp -r \"/content/drive/My Drive/WordList/labels_final\" .\n",
        "\n",
        "print(\"done download\")\n",
        "\n",
        "# drive_url = 'https://drive.google.com/file/d/1GXG9bH74UV0htJ2giYsstL3WNB-CYoJc/view?usp=sharing'\n",
        "# file_name = 'speech.gz'\n",
        "# urllib.request.urlretrieve(drive_url, file_name)\n",
        "# print('Download completed!')\n",
        "# print(\"done importing\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.12.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.19)\n",
            "Requirement already satisfied: python_speech_features in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "done download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7DiOVi-hnVP",
        "colab_type": "text"
      },
      "source": [
        "#MFCC EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQUzasCPWAZu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import python_speech_features as psf\n",
        "import soundfile as sf\n",
        "def psfmfcc(s,sr,nf=2048):\n",
        "  \n",
        "  L=psf.mfcc(s,sr,winlen=0.025,winstep=0.01,numcep=20,\n",
        "    nfilt=26,nfft=2048,lowfreq=0,highfreq=None,preemph=0.97,\n",
        "    ceplifter=22,appendEnergy=True)\n",
        "  return(L)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by3n3tDV_se3",
        "colab_type": "text"
      },
      "source": [
        "#Checking data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ql92jFgY-0rZ",
        "colab_type": "code",
        "outputId": "bdfc2718-1a40-4f03-8de8-fdf9b57e3f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "###Loading data\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas\n",
        "#load mfcc\n",
        "file=open(\"mfcc_final\",'rb')\n",
        "data=pickle.load(file)\n",
        "file.close()\n",
        "#load labels\n",
        "file=open(\"labels_final\",'rb')\n",
        "labels=pickle.load(file)\n",
        "file.close()\n",
        "lens=[len(recording) for recording in data]\n",
        "\n",
        "data=np.asarray(data)\n",
        "data.shape\n",
        "\n",
        "avg_len=np.mean(lens)\n",
        "max=np.max(lens)\n",
        "print(\"maxlen=\",max)\n",
        "std=np.std(lens)\n",
        "print(\"std=\",std)\n",
        "n_labels=len(np.unique(labels))\n",
        "\n",
        "print(\"avg len=\",avg_len)\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "\n",
        "\n",
        "print(data[1].shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "len(labels)\n",
        "\n",
        "labels[1]\n",
        "labels [10000]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maxlen= 143\n",
            "std= 15.060360756121268\n",
            "avg len= 99.36507381793211\n",
            "(163104,)\n",
            "(99, 20)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eight'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z3pqWCYusdz",
        "colab_type": "text"
      },
      "source": [
        "#Padding data\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ef4975ac-5300-4eb5-a7f7-f7de221597f2",
        "id": "iF1sWqdQTqcR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#padding our data:\n",
        "\n",
        "#We pad our data to give it the same length in order to feed it to the DNN\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "a=np.asarray(data)\n",
        "X=[]\n",
        "\n",
        "max_len=130\n",
        "\n",
        "for mfcc in a:\n",
        "  \n",
        "  mfcc=mfcc[:max_len]\n",
        "  mfcc=np.pad(mfcc,((0,max_len-len(mfcc)),(0,0)),mode='constant')\n",
        "  X.append(mfcc)\n",
        "  \n",
        "X=np.asarray(X)\n",
        "print(\"X shape is \",X.shape)\n",
        "\n",
        "#making sure data is good:\n",
        "from collections import Counter\n",
        "print(Counter(labels))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X shape is  (163104, 130, 20)\n",
            "Counter({'eight': 10194, 'five': 10194, 'four': 10194, 'nine': 10194, 'seven': 10194, 'six': 10194, 'three': 10194, 'two': 10194, 'zero': 10194, 'one': 10194, 'left': 10194, 'marvin': 10194, 'no': 10194, 'right': 10194, 'up': 10194, 'yes': 10194})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRglZ-LCu5Zt",
        "colab_type": "text"
      },
      "source": [
        "#Changing labels to numbers and splitting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zrhwnZs06sY",
        "colab_type": "code",
        "outputId": "571a9809-8597-4bd9-bd5d-302d87cfcb76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "Y=labels\n",
        "\n",
        "#giving our labels an id:\n",
        "\n",
        "dic={}\n",
        "unk=np.unique(Y)\n",
        "for i in range(len(unk)):\n",
        "  dic[unk[i]]=i\n",
        "  \n",
        "print(dic)\n",
        "n_labels=[dic[i] for i in Y]\n",
        "\n",
        "\n",
        "#creating training and testing sets:\n",
        "    \n",
        "X_train,X_test, Y_train,Y_test = train_test_split(X,n_labels,test_size=0.3)\n",
        "\n",
        "k=dic\n",
        "file=open(\"dict2\",'wb')\n",
        "pickle.dump(k,file,protocol=2)\n",
        "file.close()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "its me Counter({'eight': 10194, 'five': 10194, 'four': 10194, 'nine': 10194, 'seven': 10194, 'six': 10194, 'three': 10194, 'two': 10194, 'zero': 10194, 'one': 10194, 'left': 10194, 'marvin': 10194, 'no': 10194, 'right': 10194, 'up': 10194, 'yes': 10194})\n",
            "{'eight': 0, 'five': 1, 'four': 2, 'left': 3, 'marvin': 4, 'nine': 5, 'no': 6, 'one': 7, 'right': 8, 'seven': 9, 'six': 10, 'three': 11, 'two': 12, 'up': 13, 'yes': 14, 'zero': 15}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSn0THYdvFBT",
        "colab_type": "text"
      },
      "source": [
        "#Running code on GPU because it's faster (and cooler)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoIJGmQEsvPY",
        "colab_type": "code",
        "outputId": "15b0da0c-1951-45d8-c624-5fb7500cf1ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device_name=tf.test.gpu_device_name()\n",
        "if device_name!= '/device:GPU:0':\n",
        "     raise SystemError(\"GPU not found\")\n",
        "print('Found GPU at : {}' .format(device_name)) #checking "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at : /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx6eiR3Z3n6i",
        "colab_type": "text"
      },
      "source": [
        "#Creating and running model (DNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S3fbHvdybkk",
        "colab_type": "code",
        "outputId": "8beffefd-e857-47b4-f0ef-94951f45701d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#run code on gpu\n",
        "with tf.device('/gpu:0'):\n",
        "    model=tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(max_len, 13)),\n",
        "        tf.keras.layers.Dropout(0.20)\n",
        "        tf.keras.layers.Dense(350,activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dropout(0.18),\n",
        "        tf.keras.layers.Dense(200,activation=tf.nn.relu),\n",
        "        tf.keras.layers.Dropout(0.18),\n",
        "\n",
        "        tf.keras.layers.Dense(130,activation=tf.nn.relu,kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "\n",
        "        tf.keras.layers.Dropout(0.12),\n",
        "\n",
        "\n",
        "\n",
        "        tf.keras.layers.Dense(31,activation=tf.nn.softmax,kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "    ])\n",
        "\n",
        "\n",
        "    #compile model :\n",
        "\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # use the model:\n",
        "\n",
        "   # Y_test = to_categorical(Y_test)\n",
        "    #Y_train =to_categorical(Y_train)\n",
        "    loss,accuracy=model.evaluate(X_test,Y_test)\n",
        "    history=model.fit(X_train,Y_train,epochs=70,batch_size=90,validation_data=[X_test,Y_test])\n",
        "    loss,accuracy=model.evaluate(X_test,Y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e7e0ce83bae8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m    \u001b[0;31m# Y_test = to_categorical(Y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m#Y_train =to_categorical(Y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected flatten_1_input to have shape (130, 13) but got array with shape (130, 20)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ptpxpziJfLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tensorboard --logdir /tmp/retrain_logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdfKQ2oXAdlA",
        "colab_type": "text"
      },
      "source": [
        "#Second architecture (CNN : Best so far)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWI1u87b8b8s",
        "colab_type": "code",
        "outputId": "acc82c54-c293-4031-d40d-073a62c4e178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1528
        }
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "\n",
        "  X_train_3D = X_train.reshape(X_train.shape[0], max_len, 20, 1)\n",
        "  X_test_3D = X_test.reshape(X_test.shape[0], max_len, 20, 1)\n",
        "  print(\"shape X\" ,X_train.shape)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(60, kernel_size=(10,20), strides=(5,20), activation='relu', input_shape=(max_len, 20, 1)))\n",
        "  model.add(MaxPooling2D(pool_size=(5, 1 )))\n",
        "  model.add(Dropout(0.18))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(500, activation='relu'))\n",
        "  model.add(Dropout(0.12))\n",
        "  model.add(Dense(300, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "  model.add(Dropout(0.12))\n",
        "  \n",
        "  model.add(Dense(180, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "  model.add(Dropout(0.12))\n",
        "  model.add(Dense(100, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
        "  model.add(Dropout(0.12))\n",
        "  model.add(Dense(31, activation='softmax'))\n",
        "  model.compile(loss='sparse_categorical_crossentropy',\n",
        "                optimizer='Adam',\n",
        "                metrics=['accuracy'])\n",
        "  # simple early stopping\n",
        "  es = EarlyStopping(monitor='val_acc', mode='min', verbose=0,patience=20)\n",
        "  mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "  #train model\n",
        "  model.fit(X_train_3D, Y_train, batch_size=80, epochs=100, verbose=1, validation_data=(X_test_3D, Y_test),callbacks=[es, mc])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape X (114172, 130, 20)\n",
            "Train on 114172 samples, validate on 48932 samples\n",
            "Epoch 1/100\n",
            "113840/114172 [============================>.] - ETA: 0s - loss: 1.2544 - acc: 0.7253\n",
            "Epoch 00001: val_acc improved from -inf to 0.87707, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 11s 100us/sample - loss: 1.2528 - acc: 0.7256 - val_loss: 0.6277 - val_acc: 0.8771\n",
            "Epoch 2/100\n",
            "113760/114172 [============================>.] - ETA: 0s - loss: 0.6658 - acc: 0.8497\n",
            "Epoch 00002: val_acc improved from 0.87707 to 0.89851, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 90us/sample - loss: 0.6658 - acc: 0.8496 - val_loss: 0.4640 - val_acc: 0.8985\n",
            "Epoch 3/100\n",
            "113600/114172 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.8707\n",
            "Epoch 00003: val_acc improved from 0.89851 to 0.91476, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 88us/sample - loss: 0.5494 - acc: 0.8706 - val_loss: 0.3919 - val_acc: 0.9148\n",
            "Epoch 4/100\n",
            "113840/114172 [============================>.] - ETA: 0s - loss: 0.4941 - acc: 0.8827\n",
            "Epoch 00004: val_acc improved from 0.91476 to 0.91678, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 89us/sample - loss: 0.4943 - acc: 0.8827 - val_loss: 0.3736 - val_acc: 0.9168\n",
            "Epoch 5/100\n",
            "113760/114172 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.8904\n",
            "Epoch 00005: val_acc improved from 0.91678 to 0.92373, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 89us/sample - loss: 0.4613 - acc: 0.8904 - val_loss: 0.3399 - val_acc: 0.9237\n",
            "Epoch 6/100\n",
            "114160/114172 [============================>.] - ETA: 0s - loss: 0.4338 - acc: 0.8969\n",
            "Epoch 00006: val_acc improved from 0.92373 to 0.92530, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 88us/sample - loss: 0.4338 - acc: 0.8969 - val_loss: 0.3291 - val_acc: 0.9253\n",
            "Epoch 7/100\n",
            "113520/114172 [============================>.] - ETA: 0s - loss: 0.4174 - acc: 0.9016\n",
            "Epoch 00007: val_acc improved from 0.92530 to 0.92680, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 88us/sample - loss: 0.4175 - acc: 0.9015 - val_loss: 0.3238 - val_acc: 0.9268\n",
            "Epoch 8/100\n",
            "114080/114172 [============================>.] - ETA: 0s - loss: 0.4033 - acc: 0.9042\n",
            "Epoch 00008: val_acc improved from 0.92680 to 0.93303, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 11s 94us/sample - loss: 0.4033 - acc: 0.9041 - val_loss: 0.3020 - val_acc: 0.9330\n",
            "Epoch 9/100\n",
            "113760/114172 [============================>.] - ETA: 0s - loss: 0.3893 - acc: 0.9080\n",
            "Epoch 00009: val_acc improved from 0.93303 to 0.93307, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 92us/sample - loss: 0.3892 - acc: 0.9080 - val_loss: 0.3009 - val_acc: 0.9331\n",
            "Epoch 10/100\n",
            "113840/114172 [============================>.] - ETA: 0s - loss: 0.3764 - acc: 0.9122\n",
            "Epoch 00010: val_acc improved from 0.93307 to 0.93436, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 88us/sample - loss: 0.3766 - acc: 0.9122 - val_loss: 0.2946 - val_acc: 0.9344\n",
            "Epoch 11/100\n",
            "114160/114172 [============================>.] - ETA: 0s - loss: 0.3683 - acc: 0.9140\n",
            "Epoch 00011: val_acc improved from 0.93436 to 0.93859, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 87us/sample - loss: 0.3682 - acc: 0.9140 - val_loss: 0.2847 - val_acc: 0.9386\n",
            "Epoch 12/100\n",
            "113680/114172 [============================>.] - ETA: 0s - loss: 0.3614 - acc: 0.9164\n",
            "Epoch 00012: val_acc did not improve from 0.93859\n",
            "114172/114172 [==============================] - 10s 87us/sample - loss: 0.3617 - acc: 0.9164 - val_loss: 0.2889 - val_acc: 0.9358\n",
            "Epoch 13/100\n",
            "113920/114172 [============================>.] - ETA: 0s - loss: 0.3553 - acc: 0.9187\n",
            "Epoch 00013: val_acc did not improve from 0.93859\n",
            "114172/114172 [==============================] - 11s 96us/sample - loss: 0.3552 - acc: 0.9187 - val_loss: 0.2798 - val_acc: 0.9381\n",
            "Epoch 14/100\n",
            "114000/114172 [============================>.] - ETA: 0s - loss: 0.3469 - acc: 0.9204\n",
            "Epoch 00014: val_acc improved from 0.93859 to 0.94037, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 11s 97us/sample - loss: 0.3470 - acc: 0.9203 - val_loss: 0.2730 - val_acc: 0.9404\n",
            "Epoch 15/100\n",
            "113600/114172 [============================>.] - ETA: 0s - loss: 0.3392 - acc: 0.9227\n",
            "Epoch 00015: val_acc improved from 0.94037 to 0.94290, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 89us/sample - loss: 0.3390 - acc: 0.9228 - val_loss: 0.2636 - val_acc: 0.9429\n",
            "Epoch 16/100\n",
            "114160/114172 [============================>.] - ETA: 0s - loss: 0.3358 - acc: 0.9240\n",
            "Epoch 00016: val_acc did not improve from 0.94290\n",
            "114172/114172 [==============================] - 11s 98us/sample - loss: 0.3358 - acc: 0.9240 - val_loss: 0.2732 - val_acc: 0.9401\n",
            "Epoch 17/100\n",
            "114160/114172 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.9253\n",
            "Epoch 00017: val_acc improved from 0.94290 to 0.94300, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 89us/sample - loss: 0.3269 - acc: 0.9253 - val_loss: 0.2676 - val_acc: 0.9430\n",
            "Epoch 18/100\n",
            "114000/114172 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.9265\n",
            "Epoch 00018: val_acc did not improve from 0.94300\n",
            "114172/114172 [==============================] - 10s 87us/sample - loss: 0.3235 - acc: 0.9265 - val_loss: 0.2661 - val_acc: 0.9412\n",
            "Epoch 19/100\n",
            "113600/114172 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.9271\n",
            "Epoch 00019: val_acc did not improve from 0.94300\n",
            "114172/114172 [==============================] - 10s 87us/sample - loss: 0.3225 - acc: 0.9271 - val_loss: 0.2685 - val_acc: 0.9417\n",
            "Epoch 20/100\n",
            "113680/114172 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.9293\n",
            "Epoch 00020: val_acc improved from 0.94300 to 0.94509, saving model to best_model.h5\n",
            "114172/114172 [==============================] - 10s 88us/sample - loss: 0.3177 - acc: 0.9293 - val_loss: 0.2555 - val_acc: 0.9451\n",
            "Epoch 21/100\n",
            "114080/114172 [============================>.] - ETA: 0s - loss: 0.3102 - acc: 0.9304\n",
            "Epoch 00021: val_acc did not improve from 0.94509\n",
            "114172/114172 [==============================] - 10s 87us/sample - loss: 0.3103 - acc: 0.9304 - val_loss: 0.2710 - val_acc: 0.9419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISRse3nRvyVD",
        "colab_type": "text"
      },
      "source": [
        "#Recursive DNN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HZ7jsTVW_Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "#from tensorflow.keras.utils.np_utils import to_categorical\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# tf.keras.layers.Dense(200,activation=tf.nn.relu,kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "with tf.device('/gpu:0'):\n",
        "    model=tf.keras.models.Sequential([\n",
        "        \n",
        "        #tf.keras.layers.CuDNNLSTM(275, input_shape=(max_len,13)),\n",
        "        tf.keras.layers.LSTM(275, input_shape=(max_len,13)),\n",
        "       \n",
        "        tf.keras.layers.Dropout(0.30),\n",
        "        \n",
        "        tf.keras.layers.Dense(200,activation=tf.nn.relu,kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "\n",
        "        tf.keras.layers.Dropout(0.30),\n",
        "        tf.keras.layers.Dense(110,activation=tf.nn.relu,kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "\n",
        "        tf.keras.layers.Dropout(0.25),\n",
        "        \n",
        "        tf.keras.layers.Dense(31,activation=tf.nn.softmax,kernel_regularizer=tf.keras.regularizers.l2(0.001))\n",
        "    ])\n",
        "\n",
        "\n",
        "    #compile model :\n",
        "    opt = tf.keras.optimizers.Adam(lr = 1e-3,decay = 1e-5)\n",
        "\n",
        "    model.compile(optimizer=opt, \n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # use the model:\n",
        "\n",
        "    history=model.fit(X_train,Y_train,epochs=50,batch_size=100,validation_data=[X_test,Y_test])\n",
        "    loss,accuracy=model.evaluate(X_test,Y_test)\n",
        "    \n",
        "    from tensorflow.keras.models import save_model\n",
        "\n",
        "    # Creates a HDF5 file 'my_model.h5'\n",
        "\n",
        "\n",
        "    save_model(model,'LSTM_NORMAL.h5',True,False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMsmdZrhAZdi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CZztAxS5PQQ",
        "colab_type": "text"
      },
      "source": [
        "#Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPhSrAZ4lbE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import save_model\n",
        "\n",
        "# Creates a HDF5 file 'my_model.h5'\n",
        "\n",
        "\n",
        "save_model(model,'LSTM2.h5',True,False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EybRqwFmBFei",
        "colab_type": "text"
      },
      "source": [
        "#Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz3lKo94rF6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mo = tf.keras.models.load_model('my_model_c.h5')\n",
        "mo = tf.keras.models.load_model('my_model_c.h5')\n",
        "mo.compile(optimizer='adam', \n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy']) \n",
        "loss,accuracy=mo.evaluate(X_train,Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdkJG6dAS7Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generate confusion matrix:\n",
        "import sklearn\n",
        "Y=model.predict(X_test)\n",
        "Y_predict=[np.argmax(Y[i]) for i in range(len(Y))]\n",
        "\n",
        "sklearn.metrics.confusion_matrix(Y_test,Y_predict)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFZ-PB6-G7N5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plotting loss and accuracy over time\n",
        "history_dict = history.history\n",
        "print(history_dict.keys())\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}